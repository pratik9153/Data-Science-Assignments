{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d429ccc-ad17-4827-a6ed-1c10e1bfacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#^k What is regularization in the context of deep learningH Why is it importantG\n",
    "\n",
    "Regularization prevents overfitting in deep learning by adding constraints, improving model \n",
    "generalization and performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb7b3c-251e-42c0-8e0e-96c9bdf7b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cn Explain the concept of gradient descent and its variants. Discuss their differences and tradeoffs in terms\n",
    "#of convergence speed and memory re?uirementsn\n",
    "\n",
    "\n",
    "radient descent iteratively minimizes loss. Variants like SGD, Momentum, and Adam differ in \n",
    "convergence speed and memory use, balancing efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9804c7c-4051-4003-bd03-579e2918e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>n Describe the challenges associated with traditional gradient descent optimization methods (e.g., slow\n",
    "#convergence, local minima<. How do modern optimizers address these challengesJ\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
