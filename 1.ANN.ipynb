{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3a4b24-e518-428f-82be-77c154a79274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is an activation function in the context of artificial neural networks?\n",
    "\n",
    "Activation Function in a artifical neural network is used to introduce non-linearity in data to train more complex\n",
    "model and learn complex pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63f0a8-baa8-4dc3-8850-e9556afb0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. What are some common types of activation functions used in neural networks?\n",
    "\n",
    "sigmoid actiavtion function \n",
    "leaky relu \n",
    "relu \n",
    "parametric \n",
    "tanh \n",
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef951533-b7ed-4d7d-9fa5-23a9e52fb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. How do activation functions affect the training process and performance of a neural network?\n",
    "\n",
    "Activation functions introduce non-linearity, enabling neural networks to model complex relationships \n",
    "and improve learning efficiency and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805c88e6-15af-4dc7-aaaa-78446e5fd4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
    "Yes, that's correct. The sigmoid activation function squashes the output of a neuron between 0 and 1, \n",
    "resembling a S-shaped curve. Its advantages include output interpretation as probabilities, but it suffers \n",
    "from vanishing gradients and is not zero-centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d0e4c9-8c23-459d-9e59-ca71628cc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
    "\n",
    "Relu is a type of actiavtion function that in nn , setting negative values to zeros . sigmoid function squashes the \n",
    "output between 0 and 1 other hand relu set negative value to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b6017-8161-47da-b278-0d1f201d7413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. What are the benefits of using the ReLU activation function over the sigmoid function?\n",
    "\n",
    "Using the ReLU activation function offers faster convergence during training due to non-saturation, \n",
    "simpler computation, and mitigated vanishing gradient problems compared to sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257768f5-bbe3-4f8f-a15d-b2b244e7ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n",
    "\n",
    "Leaky ReLU is a variant of the ReLU activation function that allows a small, non-zero gradient when the \n",
    "input is negative, preventing dead neurons and mitigating the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925001f-729f-4503-acec-9cbeb23263a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. What is the purpose of the softmax activation function? When is it commonly used?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
