{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55352e-7269-4d55-9045-714c1430baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1.\n",
    "\n",
    "    Overfitting: High accuracy on training data, low accuracy on testing data, low bias, high variance.\n",
    "    \n",
    "    Underfitting: Low accuracy on both training and testing data, high bias, low variance.\n",
    "    \n",
    "    consequences:-\n",
    "    1.poor generalization : The overfitted model may perform well on training data but fail to generalize unseen \n",
    "    data that leads to inaccurate pridictions.\n",
    "    \n",
    "    2.Increased complexity: overfitted model tend to overly complex ,making them harder to interpret and maintain.\n",
    "    \n",
    "    Mitigation :\n",
    "        \n",
    "    1.Feature selection/reduction: Reduce the number of features or perform feature selection to focus on the most \n",
    "     informative features and avoid overfitting to noise.\n",
    "        \n",
    "    2.Early stopping: Monitor the models performance on a separate validation dataset during training and stop\n",
    "    training when performance starts to degrade.\n",
    "    \n",
    "    consequences:-\n",
    "    \n",
    "    1.Limited predictive power: Models with high bias (underfitted) lack the complexity to capture the true \n",
    "    relationships in the data, leading to inaccurate predictions.\n",
    "    \n",
    "    2.Missed opportunities: Underfitting may result in missed opportunities to leverage the full potential of the data \n",
    "    for making accurate predictions.\n",
    "    \n",
    "    Mitigation:\n",
    "        \n",
    "    1.Increase model complexity: Use more complex models or increase the complexity of existing models to better \n",
    "        capture the underlying patterns in the data.\n",
    "        \n",
    "    2.Feature engineering: Create new features or transform existing ones to provide the model with more information to \n",
    "      learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d06dc-8659-40d6-a9dd-98b754b4c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2.\n",
    "\n",
    "Employ regularization techniques like L1/L2 regularization.\n",
    "Use cross-validation to assess model performance.\n",
    "Limit model complexity via feature selection or reduction.\n",
    "Implement early stopping during training.\n",
    "Augment data or use dropout regularization.\n",
    "Utilize ensemble methods to combine multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ced34-52b2-4a66-80cc-2c16f7f4421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3.\n",
    "\n",
    "Underfitting: Low accuracy on both training and testing data, high bias, low variance.\n",
    "\n",
    "    The model is too simple or lacks complexity to capture the underlying patterns in the data.\n",
    "    There is insufficient training data available.\n",
    "    Features are poorly chosen or do not adequately represent the underlying relationships.\n",
    "    Model hyperparameters are not tuned appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db8bb9-8d11-437c-b22e-70d3e6f466d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4.\n",
    "\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that describe the relationship between\n",
    "bias of a model and its variance and how they collectively affect the model performance.\n",
    "\n",
    "Bias refers to the error introduced by approximating a real-world problem with a simplified model. High bias \n",
    "implies that the model makes strong assumptions about the data, often resulting in oversimplified representations \n",
    "and underfitting. In essence, a biased model fails to capture the underlying patterns in the data.\n",
    "\n",
    "Variance, on the other hand, refers to the models sensitivity to fluctuations in the training data.  \n",
    "High variance indicates that the model is too complex and captures noise in the training data, \n",
    "leading to overfitting. Such models perform well on training data but generalize poorly to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e77db-9a77-42ad-a9f5-5d90f9968406",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5.\n",
    " \n",
    "methods for detecting overfitting and underfitting in machine learning models:-\n",
    "\n",
    " Cross-Validation: Using techniques like k-fold cross-validation helps evaluate model performance on multiple \n",
    " subsets of the data.\n",
    " \n",
    " Model Complexity Analysis: Experimenting with different model complexities \n",
    " and observing how performance changes on the testing dataset can help identify overfitting or underfitting.\n",
    "    \n",
    " Regularization Parameter Tuning: Adjusting regularization parameters (e.g., L1/L2 regularization strength) and \n",
    "observing the impact on performance can indicate whether the model is overfitting or underfitting.\n",
    "    \n",
    "To determine whether a model is overfitting or underfitting, compare its performance on the training and \n",
    "testing datasets. Overfitting typically exhibits high performance on the training dataset but lower performance \n",
    "on the testing dataset. Underfitting, on the other hand, results in poor performance on both training and \n",
    "testing datasets. By employing these methods, practitioners can diagnose and address overfitting and \n",
    "underfitting to improve model generalization and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbe4ce-d056-43a5-a210-4091f7acaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6\n",
    "Bias refers to error from oversimplification, while variance indicates sensitivity to training data. High bias \n",
    "models, like linear regression, underfit data. High variance models, like deep neural networks, overfit, \n",
    "performing well on training but poorly on testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacbdcec-6c84-4899-915d-2dccae47176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7\n",
    "Regularization in machine learning is a technique to prevent overfitting by adding a penalty \n",
    "term to the models cost function. Common techniques include L1 (Lasso) and L2 (Ridge) regularization, \n",
    "which add the absolute or  squared sum of model weights to the cost function, respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
